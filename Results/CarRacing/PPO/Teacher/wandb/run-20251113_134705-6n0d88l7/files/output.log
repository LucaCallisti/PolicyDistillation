Start training...
Warning: Ratio not close to 1 at first update! elements differ. 1.0 max diff.
Sanity check for NaNs in model parameters, iteration: 1
Sanity check - CNN params sum: 345.74685509306323
Sanity check - critic params sum: -20.230171789877204
Sanity check - actor_backbone params sum: 29.926914304494858
Sanity check - mu_layer params sum: 0.014630310237407684
Sanity check - log_std_param params sum: -3.624745726585388
[1/2000] AvgRew: nan
/home/l.callisti/Distillation/distillation/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at /home/l.callisti/Distillation_LunarLander/Final_pipeline/Results/CarRacing/PPO/Teacher/videos/PPO_CarRacing_Teacher_BatchNorm folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
Finished with reward: -16.6, Reward per episode: [ -3.7 -24.1  -3.2 -17.3 -12.8  -7.1  -9.8 -32.6 -24.2 -31.1]
Warning: Ratio not close to 1 at first update! elements differ. inf max diff.
/home/l.callisti/Distillation/distillation/lib/python3.10/site-packages/torch/autograd/graph.py:824: UserWarning: Error detected in ExpBackward0. Traceback of forward call that caused the error:
  File "/home/l.callisti/Distillation_LunarLander/Final_pipeline/CarRacing.py", line 116, in <module>
    Train_teacher()
  File "/home/l.callisti/Distillation_LunarLander/Final_pipeline/CarRacing.py", line 112, in Train_teacher
    PPO_trainer.train()
  File "/home/l.callisti/Distillation_LunarLander/Final_pipeline/PPO.py", line 335, in train
    logs = self.ppo_update(advantages, returns)
  File "/home/l.callisti/Distillation_LunarLander/Final_pipeline/PPO.py", line 203, in ppo_update
    ratio = logratio.exp()
 (Triggered internally at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:122.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
> /home/l.callisti/Distillation_LunarLander/Final_pipeline/PPO.py(254)ppo_update()
-> grad_norm = nn.utils.clip_grad_norm_(self.agent.parameters(), cfg["max_grad_norm"])
tensor(inf, device='cuda:4', grad_fn=<AddBackward0>)
tensor(inf, device='cuda:4', grad_fn=<MeanBackward0>)
tensor(10.2568, device='cuda:4', grad_fn=<MeanBackward0>)
tensor(0.7877, device='cuda:4', grad_fn=<MulBackward0>)
tensor([inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
        inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
        inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
        inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
        inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
        inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
        inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
        inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
        inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
        inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
        inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],
       device='cuda:4', grad_fn=<ExpBackward0>)
tensor([3238.1245, 3208.0530, 3305.2271, 3109.8411, 3289.8545, 3216.8545,
        3268.1035, 3192.7554, 3177.4993, 3267.3259, 3240.2134, 3284.1069,
        3178.0020, 3189.6338, 3270.6833, 3253.4939, 3218.1133, 3289.8545,
        3254.9543, 3278.3735, 3200.1323, 3288.0039, 3247.0781, 3199.9937,
        3205.5354, 3247.4595, 3253.8533, 3177.4993, 3203.3506, 3334.3879,
        3241.3892, 3180.9124, 3221.6624, 3085.5847, 3315.6360, 3245.5764,
        3238.6877, 3171.4109, 3224.6445, 3205.1038, 3297.8555, 3253.3350,
        3283.0742, 3224.7983, 3252.7419, 3256.2258, 3232.2280, 3264.2185,
        3183.2043, 3183.9797, 3195.5076, 3214.4302, 3244.9011, 3303.6558,
        3232.1663, 3205.5354, 3271.7029, 3244.9011, 3166.0264, 3268.5117,
        3184.2329, 3261.1213, 3228.9314, 3294.0015, 3255.9927, 3284.5815,
        3205.2302, 3204.4600, 3243.8618, 3252.7419, 3183.5688, 3201.1570,
        3268.1960, 3264.1665, 3271.3362, 3195.3877, 3267.5361, 3167.8477,
        3210.6904, 3228.5750, 3231.4744, 3249.1201, 3204.2620, 3296.6206,
        3234.1372, 3245.0178, 3297.7129, 3185.6331, 3200.1304, 3220.6436,
        3230.1985, 3195.3877, 3228.8337, 3241.3240, 3130.3513, 3254.8257,
        3192.7554, 3199.3816, 3209.5403, 3251.1587, 3169.6345, 3216.6597,
        3223.2637, 3220.9446, 3262.3218, 3168.4758, 3185.8970, 3258.5437,
        3200.8047, 3217.8567, 3209.7979, 3195.0481, 3254.9543, 3231.4744,
        3294.0015, 3133.7832, 3238.5413, 3261.1213, 3226.8289, 3214.5476,
        3211.9946, 3284.1069, 3235.9736, 3184.2329, 3185.6331, 3192.2607,
        3234.6980, 3298.5593, 3186.8376, 3200.2764, 3196.7136, 3133.5820,
        3196.3555, 3229.4814, 3198.2241, 3254.9543, 3218.5706, 3247.0781,
        3294.0015, 3233.0525, 3205.2922, 3244.3733, 3241.0269, 3221.6318,
        3253.0366, 3337.4937, 3200.2764, 3260.1990, 3262.4199, 3230.9563,
        3296.1846, 3174.9829, 3294.0015, 3215.4021, 3201.5305, 3240.2300,
        3199.0215, 3246.4248, 3355.2161, 3188.7295, 3195.1550, 3267.4343,
        3234.1372, 3179.0791, 3243.4863, 3276.6458, 3229.6616, 3238.6877,
        3228.5750, 3226.2786, 3274.0474, 3274.1226, 3271.3362, 3176.4211,
        3219.5791, 3226.2786, 3226.8289, 3206.1328, 3215.4021, 3229.0137,
        3337.4937, 3245.8140, 3191.3071, 3185.8970, 3230.1985, 3199.0215,
        3304.0120, 3215.7769, 3294.4463, 3229.9766, 3255.4731, 3172.5508,
        3266.8054, 3215.6809, 3295.2241, 3217.9841, 3272.9229, 3201.1570,
        3219.8149, 3252.2000, 3190.7942, 3224.7983, 3294.0015, 3241.1313,
        3178.6304, 3188.7295, 3180.0615, 3232.2280, 3249.1201, 3207.3030,
        3173.9761, 3271.7029, 3217.4092, 3170.9966, 3160.0156, 3206.9233,
        3227.5537, 3174.3240, 3231.4744, 3257.8184, 3236.1472, 3169.6345,
        3295.5044, 3268.1035, 3171.4109, 3258.1248, 3254.5723, 3262.8186,
        3296.1846, 3267.4578, 3261.1213, 3215.9475, 3251.1587, 3266.8054,
        3183.3909, 3201.2063, 3229.6616, 3256.2258, 3240.2981, 3262.7769,
        3217.8567, 3235.0581, 3191.3071, 3239.7800, 3255.0579, 3234.1372,
        3222.0417, 3196.7136, 3262.8186, 3266.9614, 3220.9446, 3320.7073,
        3226.8289, 3251.1587, 3247.8093, 3316.5288], device='cuda:4',
       grad_fn=<SubBackward0>)
tensor(3687.1768, device='cuda:4', grad_fn=<SumBackward1>)
tensor(3596.0479, device='cuda:4', grad_fn=<SumBackward1>)
Traceback (most recent call last):
  File "/home/l.callisti/Distillation_LunarLander/Final_pipeline/CarRacing.py", line 116, in <module>
    Train_teacher()
  File "/home/l.callisti/Distillation_LunarLander/Final_pipeline/CarRacing.py", line 112, in Train_teacher
    PPO_trainer.train()
  File "/home/l.callisti/Distillation_LunarLander/Final_pipeline/PPO.py", line 335, in train
    logs = self.ppo_update(advantages, returns)
  File "/home/l.callisti/Distillation_LunarLander/Final_pipeline/PPO.py", line 254, in ppo_update
    grad_norm = nn.utils.clip_grad_norm_(self.agent.parameters(), cfg["max_grad_norm"])
  File "/home/l.callisti/Distillation_LunarLander/Final_pipeline/PPO.py", line 254, in ppo_update
    grad_norm = nn.utils.clip_grad_norm_(self.agent.parameters(), cfg["max_grad_norm"])
  File "/usr/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/usr/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
